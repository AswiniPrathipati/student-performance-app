# -*- coding: utf-8 -*-
"""Student Academic Performance Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11vwUbt45KL-02xkUU7TgEMbg7nC2xBAL

#Step 1: Load Libraries & Dataset
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('student-mat.csv', sep=';')

# Display first 5 rows
print(df.head())

"""**Code Summary:** Loaded student-mat.csv using pandas, and displayed the first few rows.



**Interpretation:** The dataset was successfully loaded. It contains student demographics, behaviors, and grades (G1, G2, G3).

#Step 2: Data Exploration
"""

# Shape of the data
print("Shape of the dataset:", df.shape)

# Column names
print("Columns:", df.columns.tolist())

# Data types
print("\nData Types:\n", df.dtypes)

# Summary statistics
print("\nSummary:\n", df.describe())

# Check for missing values
print("\nMissing values:\n", df.isnull().sum())

"""**Code Summary:** Checked dataset shape, columns, types, summary statistics, and missing values.



**Interpretation:** The dataset has no missing values. It includes 395 rows and 33 features, with a mix of numeric and categorical data.

#Step 3: Data Preprocessing
"""

from sklearn.preprocessing import LabelEncoder

# Copy dataframe to avoid modifying original
data = df.copy()

# Encode categorical variables
categorical_cols = data.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols.tolist())

le = LabelEncoder()
for col in categorical_cols:
    data[col] = le.fit_transform(data[col])

print("\nData after encoding (first 5 rows):")
print(data.head())

"""**Code Summary:** Used LabelEncoder to convert categorical variables to numerical ones.



**Interpretation:** This step prepares categorical data for modeling, ensuring all features are numeric and machine learning–ready.

# Step 4: Exploratory Data Analysis (EDA)
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Correlation heatmap
plt.figure(figsize=(14,10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Boxplot: Study time vs Final Grade
plt.figure(figsize=(8,5))
sns.boxplot(x='studytime', y='G3', data=data)
plt.title('Study Time vs Final Grade')
plt.xlabel('Study Time (1= <2h, 2= 2-5h, 3=5-10h, 4= >10h)')
plt.ylabel('Final Grade (G3)')
plt.show()

# Boxplot: Failures vs Final Grade
plt.figure(figsize=(8,5))
sns.boxplot(x='failures', y='G3', data=data)
plt.title('Failures vs Final Grade')
plt.xlabel('Number of Past Class Failures')
plt.ylabel('Final Grade (G3)')
plt.show()

# Scatterplot: Absences vs Final Grade
plt.figure(figsize=(8,5))
sns.scatterplot(x='absences', y='G3', data=data)
plt.title('Absences vs Final Grade')
plt.xlabel('Number of Absences')
plt.ylabel('Final Grade (G3)')
plt.show()

# Alcohol consumption vs Final Grade (average of weekday and weekend alcohol consumption)
data['alc_avg'] = (data['Dalc'] + data['Walc']) / 2
plt.figure(figsize=(8,5))
sns.boxplot(x='alc_avg', y='G3', data=data)
plt.title('Average Alcohol Consumption vs Final Grade')
plt.xlabel('Average Alcohol Consumption (0=very low to 5=very high)')
plt.ylabel('Final Grade (G3)')
plt.show()

"""**Code Summary:** Correlation heatmap, boxplot for studytime vs G3, and scatterplot for absences vs G3.



**Interpretation:**

G1 and G2 strongly correlate with G3.

More study time usually leads to higher final grades.

More absences are associated with lower final grades.

#Step 5: Model Building

**1. Linear Regression**
"""

#Linear Regression


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Using the preprocessed data 'data' from previous step

# Separate features and target
X = data.drop(['G3', 'alc_avg'], axis=1)  # Drop target and the temporary alc_avg column
y = data['G3']

# Split into training and testing data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize and train Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predict on test set
y_pred = lr_model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Linear Regression Model Performance:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"R-squared (R2 ): {r2:.2f}")

"""**Code Summary:** Split data into train/test sets, trained a Linear Regression model, and made predictions.



**Interpretation:** This step established a baseline model with simple linear relationships. Good for comparison with complex models.

**2. Random Forest**
"""

from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"\nRandom Forest Model Performance:")
print(f"MSE: {mse_rf:.2f}")
print(f"MAE: {mae_rf:.2f}")
print(f"R² Score: {r2_rf:.2f}")

"""**Code Summary:**Trained a RandomForestRegressor and evaluated performance using MSE, MAE, and R².



**Interpretation:** The Random Forest model performed better than linear regression, indicating non-linear relationships in the data.

**3. XGBoost**
"""

# XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

mse_xgb = mean_squared_error(y_test, y_pred_xgb)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f"\nXGBoost Model Performance:")
print(f"MSE: {mse_xgb:.2f}")
print(f"MAE: {mae_xgb:.2f}")
print(f"R² Score: {r2_xgb:.2f}")

"""**Code Summary:** Trained an XGBRegressor and evaluated its performance using the same metrics.



**Interpretation:** XGBoost gave competitive results but was slightly outperformed by Random Forest in this case.

#Step 6: Model Evaluation & Comparison
"""

import pandas as pd
import matplotlib.pyplot as plt

# Prepare a summary dataframe
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],
    'MSE': [mse, mse_rf, mse_xgb],
    'MAE': [mae, mae_rf, mae_xgb],
    'R2_Score': [r2, r2_rf, r2_xgb]
})

print("Model Performance Comparison:\n", results)

# Plot R2 Scores for visual comparison
plt.figure(figsize=(8,5))
sns.barplot(x='Model', y='R2_Score', data=results)
plt.title('Model Comparison: R² Score')
plt.ylim(0,1)
plt.show()

"""**Code Summary:**Compared MSE, MAE, and R² of all three models.



**Interpretation:**

Random Forest achieved the highest R² (~0.83) and lowest error, making it the best-performing model.

Linear Regression had the lowest performance.

XGBoost was in between.

#Step 7: Residual Analysis and Error Diagnostics

1. Residual Plot
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assume y_test and y_pred_rf (Random Forest predictions) already exist
residuals = y_test - y_pred_rf

plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_pred_rf, y=residuals)
plt.axhline(0, linestyle='--', color='red')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot - Random Forest")
plt.show()

"""2. Histogram of Residuals"""

plt.figure(figsize=(8, 5))
sns.histplot(residuals, kde=True, bins=30)
plt.title("Histogram of Residuals")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.show()

"""3. QQ Plot"""

import scipy.stats as stats
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("QQ Plot of Residuals")
plt.show()

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import pickle

# ✅ FIX: Use correct delimiter
df = pd.read_csv("student-mat.csv", delimiter=';')

# Use only these 5 features (they exist now as separate columns)
X = df[["studytime", "failures", "absences", "G1", "G2"]]
y = df["G3"]

# Train the model
model = RandomForestRegressor()
model.fit(X, y)

# Save the model
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)